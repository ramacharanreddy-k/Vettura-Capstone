{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import json\n",
    "import boto3\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "import time\n",
    "from typing import List, Dict, Any\n",
    "import pandas as pd\n",
    "from IPython.display import display, HTML, Image\n",
    "\n",
    "# Load environment variables\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configure your settings here\n",
    "KB_ID = \"PXQS78QKBV\"  # Your knowledge base ID\n",
    "LLM_MODEL_ID = \"us.meta.llama3-2-11b-instruct-v1:0\"  # LLM model for response generation\n",
    "EMBEDDING_MODEL_ID = \"amazon.titan-embed-text-v2:0\"  # Embedding model\n",
    "REGION = \"us-east-1\"  # AWS region"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialized Bedrock clients with Knowledge Base ID: PXQS78QKBV\n",
      "LLM Model: us.meta.llama3-2-11b-instruct-v1:0\n",
      "Embedding Model: amazon.titan-embed-text-v2:0\n"
     ]
    }
   ],
   "source": [
    "# Initialize Bedrock clients\n",
    "bedrock_runtime = boto3.client(\n",
    "    'bedrock-runtime',\n",
    "    region_name=REGION,\n",
    "    aws_access_key_id=os.environ.get('Home_AWS_ACCESS_KEY_ID'),\n",
    "    aws_secret_access_key=os.environ.get('Home_AWS_SECRET_ACCESS_KEY')\n",
    ")\n",
    "\n",
    "bedrock_agent = boto3.client(\n",
    "    'bedrock-agent-runtime',\n",
    "    region_name=REGION,\n",
    "    aws_access_key_id=os.environ.get('Home_AWS_ACCESS_KEY_ID'),\n",
    "    aws_secret_access_key=os.environ.get('Home_AWS_SECRET_ACCESS_KEY')\n",
    ")\n",
    "\n",
    "print(f\"Initialized Bedrock clients with Knowledge Base ID: {KB_ID}\")\n",
    "print(f\"LLM Model: {LLM_MODEL_ID}\")\n",
    "print(f\"Embedding Model: {EMBEDDING_MODEL_ID}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_embedding(text):\n",
    "    \"\"\"Generate embedding for a text using Bedrock Titan Embedding model.\"\"\"\n",
    "    if not text:\n",
    "        return []\n",
    "        \n",
    "    try:\n",
    "        # Prepare the request body for the model\n",
    "        request_body = json.dumps({\n",
    "            \"inputText\": text\n",
    "        })\n",
    "        \n",
    "        # Call the Bedrock runtime\n",
    "        response = bedrock_runtime.invoke_model(\n",
    "            modelId=EMBEDDING_MODEL_ID,\n",
    "            body=request_body\n",
    "        )\n",
    "        \n",
    "        # Process the response\n",
    "        response_body = json.loads(response.get('body').read())\n",
    "        embedding = response_body.get('embedding')\n",
    "        \n",
    "        return embedding\n",
    "    except Exception as e:\n",
    "        print(f\"Error generating embedding: {str(e)}\")\n",
    "        return []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def retrieve_from_kb(query, top_k=6, score_threshold=0.6):\n",
    "    \"\"\"Retrieve relevant content from Knowledge Base based on query.\"\"\"\n",
    "    try:\n",
    "        # Retrieval using text query\n",
    "        print(\"Retrieving from Knowledge Base...\")\n",
    "        response = bedrock_agent.retrieve(\n",
    "            knowledgeBaseId=KB_ID,\n",
    "            retrievalQuery={\n",
    "                \"text\": query\n",
    "            },\n",
    "            retrievalConfiguration={\n",
    "                \"vectorSearchConfiguration\": {\n",
    "                    \"numberOfResults\": top_k\n",
    "                }\n",
    "            }\n",
    "        )\n",
    "        \n",
    "        # Process retrieved results\n",
    "        retrieved_results = response.get(\"retrievalResults\", [])\n",
    "        print(f\"Retrieved {len(retrieved_results)} results in total\")\n",
    "        \n",
    "        # Print first result metadata to debug\n",
    "        if retrieved_results:\n",
    "            print(f\"First result metadata sample: {json.dumps(retrieved_results[0].get('metadata', {}), indent=2)}\")\n",
    "        \n",
    "        # Filter by score threshold and sort by relevance\n",
    "        filtered_results = []\n",
    "        for result in retrieved_results:\n",
    "            # Extract content and metadata\n",
    "            content = result.get(\"content\", {}).get(\"text\", \"\")\n",
    "            metadata = result.get(\"metadata\", {})\n",
    "            score = result.get(\"score\", 0)\n",
    "            \n",
    "            # Skip if score is below threshold\n",
    "            if score < score_threshold:\n",
    "                continue\n",
    "            \n",
    "            filtered_results.append({\n",
    "                \"content\": content,\n",
    "                \"metadata\": metadata,\n",
    "                \"score\": score\n",
    "            })\n",
    "        \n",
    "        # Sort by score\n",
    "        filtered_results = sorted(filtered_results, key=lambda x: x[\"score\"], reverse=True)\n",
    "        \n",
    "        print(f\"Filtered to {len(filtered_results)} results above threshold {score_threshold}\")\n",
    "        return {\n",
    "            \"results\": filtered_results\n",
    "        }\n",
    "    except Exception as e:\n",
    "        print(f\"Error retrieving from Knowledge Base: {str(e)}\")\n",
    "        import traceback\n",
    "        traceback.print_exc()\n",
    "        return {\"results\": []}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "def generate_llm_response(query, retrieved_data):\n",
    "    \"\"\"Generate response using Bedrock LLM with retrieved data.\"\"\"\n",
    "    try:\n",
    "        # Create context from retrieved data\n",
    "        context = \"I'm providing you with relevant information to answer the query.\\n\\n\"\n",
    "        \n",
    "        # Add information from all results\n",
    "        if retrieved_data[\"results\"]:\n",
    "            context += \"RELEVANT INFORMATION:\\n\"\n",
    "            for i, result in enumerate(retrieved_data[\"results\"], 1):\n",
    "                context += f\"Source {i}:\\n\"\n",
    "                if \"title\" in result[\"metadata\"]:\n",
    "                    context += f\"Title: {result['metadata'].get('title', 'Untitled')}\\n\"\n",
    "                context += f\"Content: {result['content']}\\n\\n\"\n",
    "        \n",
    "        # Create prompt for LLM with very specific instructions\n",
    "        prompt = f\"\"\"\n",
    "Context:\n",
    "{context}\n",
    "\n",
    "User Query: {query}\n",
    "\n",
    "IMPORTANT INSTRUCTIONS FOR RESPONSE:\n",
    "1. Answer the query in 2-4 sentences maximum.\n",
    "2. Include only essential information directly related to the query.\n",
    "3. Do not repeat any information.\n",
    "4. Do not use any formatting like tables, bullets, or separator lines.\n",
    "5. Do not include any disclaimers, follow-ups, or offers of additional help.\n",
    "6. Provide just the direct answer to the query and nothing more.\n",
    "\n",
    "Answer:\n",
    "\"\"\"\n",
    "        print(\"Generating response with LLM...\")\n",
    "        # Call Bedrock LLM model with lower max_gen_len\n",
    "        request_body = json.dumps({\n",
    "            \"prompt\": prompt,\n",
    "            \"max_gen_len\": 128,  # Very limited to prevent repetition\n",
    "            \"temperature\": 0.01,  # Nearly deterministic\n",
    "            \"top_p\": 0.1  # Very focused sampling\n",
    "        })\n",
    "        \n",
    "        # Invoke the model\n",
    "        response = bedrock_runtime.invoke_model(\n",
    "            modelId=LLM_MODEL_ID,\n",
    "            body=request_body\n",
    "        )\n",
    "        \n",
    "        # Parse the response\n",
    "        response_body = json.loads(response.get('body').read())\n",
    "        llm_response = response_body.get('generation', '')\n",
    "        \n",
    "        # Clean up the response\n",
    "        # 1. Remove table formatting\n",
    "        llm_response = llm_response.split('|')[0]\n",
    "        llm_response = llm_response.replace('---', '')\n",
    "        \n",
    "        # 2. Remove repetitive sentences by splitting into sentences and removing duplicates\n",
    "        sentences = [s.strip() for s in re.split(r'[.!?]+', llm_response) if s.strip()]\n",
    "        unique_sentences = []\n",
    "        for sentence in sentences:\n",
    "            if sentence not in unique_sentences:\n",
    "                unique_sentences.append(sentence)\n",
    "        \n",
    "        # 3. Reconstruct the response with only unique sentences\n",
    "        cleaned_response = '. '.join(unique_sentences[:4])  # Limit to 4 sentences\n",
    "        if cleaned_response and not cleaned_response.endswith(('.', '!', '?')):\n",
    "            cleaned_response += '.'\n",
    "        \n",
    "        return cleaned_response\n",
    "    except Exception as e:\n",
    "        print(f\"Error generating LLM response: {str(e)}\")\n",
    "        return f\"I'm sorry, I couldn't generate a response due to an error: {str(e)}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "def answer_query(query):\n",
    "    \"\"\"End-to-end function to answer a user query using RAG.\"\"\"\n",
    "    print(f\"Processing query: {query}\")\n",
    "    \n",
    "    # Step 1: Retrieve relevant information from Knowledge Base\n",
    "    print(\"Retrieving relevant information...\")\n",
    "    retrieved_data = retrieve_from_kb(query)\n",
    "    \n",
    "    # Step 2: Generate response using LLM\n",
    "    print(\"Generating response...\")\n",
    "    response = generate_llm_response(query, retrieved_data)\n",
    "    \n",
    "    # Return response with retrieved data for transparency\n",
    "    return {\n",
    "        \"query\": query,\n",
    "        \"response\": response,\n",
    "        \"retrieved_data\": retrieved_data\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "def display_results(result):\n",
    "    \"\"\"Display the results in a nice format\"\"\"\n",
    "    # Display the response\n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(f\"QUERY: {result['query']}\")\n",
    "    print(\"=\"*80)\n",
    "    print(f\"RESPONSE:\\n{result['response']}\")\n",
    "    print(\"=\"*80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing query: How to find RH850 device file?\n",
      "Retrieving relevant information...\n",
      "Retrieving from Knowledge Base...\n",
      "Retrieved 6 results in total\n",
      "First result metadata sample: {\n",
      "  \"x-amz-bedrock-kb-source-uri\": \"s3://renesas-rag/kb-data/bedrock_kb_format.json\",\n",
      "  \"x-amz-bedrock-kb-chunk-id\": \"1%3A0%3AweNke5UBsA7k3aYZmfcA\",\n",
      "  \"x-amz-bedrock-kb-data-source-id\": \"VM0LEKRLQX\"\n",
      "}\n",
      "Filtered to 3 results above threshold 0.6\n",
      "Generating response...\n",
      "Generating response with LLM...\n",
      "\n",
      "================================================================================\n",
      "QUERY: How to find RH850 device file?\n",
      "================================================================================\n",
      "RESPONSE:\n",
      "The RH850 device file can be found in the install directory, specifically in the path C:\\Program Files (x86)\\Renesas Electronics\\CS\\Device\\RH850\\Devicefile. It is not necessary to select the file manually, as it is done automatically if a device is selected in the CS Device Selection dialogue. If the device is not listed, check that you are using the latest version of CS. The device file is a binary file that describes the device and is used to create header-files for a C-compiler and other files.\n",
      "================================================================================\n",
      "\n",
      "Total processing time: 1.66 seconds\n"
     ]
    }
   ],
   "source": [
    "# Execute a query and measure time\n",
    "start_time = time.time()\n",
    "query = \"How to find RH850 device file?\"\n",
    "result = answer_query(query)\n",
    "end_time = time.time()\n",
    "\n",
    "# Display results\n",
    "display_results(result)\n",
    "print(f\"\\nTotal processing time: {end_time - start_time:.2f} seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
