# Web Scraping and Data Processing Pipeline

This repository contains a set of Python scripts designed to scrape data from a website, process it, and convert it into structured formats like CSV and JSON. The pipeline consists of three main scripts:

1. **`categoryExtract.py`**: Extracts categories and subcategories from a website.
2. **`subCategoryExtract.py`**: Extracts article URLs from subcategories.
3. **`extractJson.py`**: Converts the scraped data into a nested JSON structure.

## Prerequisites

Before running the scripts, ensure you have the following installed:

- Python 3.x
- Selenium
- ChromeDriver (automatically managed by `webdriver_manager`)
- Required Python packages (install via `pip install -r requirements.txt`)

You can install the required packages using:

```bash
pip install selenium webdriver-manager csv logging datetime time
```

## Scripts Overview

### 1. `categoryExtract.py`

This script is responsible for extracting categories and subcategories from the target website. It uses Selenium to navigate the website and extract the necessary data.

#### Key Features:
- **Headless Browser**: Runs Chrome in headless mode for efficient scraping.
- **Logging**: Logs all actions and errors for debugging and monitoring.
- **CSV Output**: Saves the extracted data into a CSV file.

#### Usage:

```bash
python categoryExtract.py
```

#### Output:
- A CSV file (`categories_data.csv`) containing the extracted categories and subcategories.

### 2. `subCategoryExtract.py`

This script takes the CSV file generated by `categoryExtract.py` and extracts article URLs from each subcategory. It then appends these URLs to the existing CSV file.

#### Key Features:
- **Article URL Extraction**: Extracts all article URLs from subcategory pages.
- **CSV Update**: Updates the existing CSV file with the extracted article URLs.

#### Usage:

```bash
python subCategoryExtract.py
```

#### Output:
- An updated CSV file (`categories_with_topics.csv`) containing the original data plus the article URLs.

### 3. `extractJson.py`

This script converts the final CSV file (`categories_with_topics.csv`) into a nested JSON structure, which is more suitable for hierarchical data representation.

#### Key Features:
- **CSV to JSON Conversion**: Converts the CSV data into a nested JSON format.
- **Hierarchical Structure**: Represents categories, subcategories, and articles in a nested structure.

#### Usage:

```bash
python extractJson.py
```

#### Output:
- A JSON file (`websiteMap.json`) containing the nested structure of categories, subcategories, and articles.

## File Structure

```
.
├── Data/
│   ├── categories_data.csv          # Output from categoryExtract.py
│   ├── categories_with_topics.csv    # Output from subCategoryExtract.py
│   └── websiteMap.json               # Output from extractJson.py
├── logs/
│   └── crawler_*.log                 # Log files generated by the scripts
├── categoryExtract.py                # Script to extract categories and subcategories
├── subCategoryExtract.py             # Script to extract article URLs
├── extractJson.py                    # Script to convert CSV to JSON
└── README.md                         # This file
```

## Running the Pipeline

1. **Extract Categories and Subcategories**:
   ```bash
   python categoryExtract.py
   ```

2. **Extract Article URLs**:
   ```bash
   python subCategoryExtract.py
   ```

3. **Convert to JSON**:
   ```bash
   python extractJson.py
   ```

## Logging

All scripts generate log files in the `logs/` directory. These logs are useful for debugging and tracking the progress of the scraping process.

## Customization

- **Target Website**: Modify the `BASE_URL` in `categoryExtract.py` to scrape a different website.
- **Selectors**: Adjust the CSS selectors in the scripts if the website structure changes.
- **Output Paths**: Change the output file paths in the scripts if needed.

## Troubleshooting

- **WebDriver Issues**: Ensure that ChromeDriver is correctly installed and matches your Chrome browser version.
- **Dynamic Content**: If the website uses heavy JavaScript, you may need to adjust the waiting times in the scripts.
- **Logs**: Check the log files for detailed error messages and debugging information.

## License

This project is licensed under the MIT License. See the [LICENSE](LICENSE) file for details.

---

This README provides an overview of the scripts and how to use them. For more detailed information, refer to the comments and documentation within each script.